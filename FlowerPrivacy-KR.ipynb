{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FlowerPrivacy-KR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "29d3d96e88912cf822d05a46426b769de26102bc74c7803e89f30736ce2d7191"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('kaicd-fl-poc-notebooks-GIMwPCRC-py3.8': poetry)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz71fPGrpRiQ"
      },
      "source": [
        "# Opacus를 활용한 Flower 연합학습 차등 개인정보보호\n",
        "\n",
        "이번 튜토리얼에서는 개인정보보호 연합학습을 위해 PyTorch와 Flower를 사용해서 Opacus를 활용하는 방법을 알아보는 시간을 갖겠습니다.\n",
        "\n",
        "아래의 코드들은 이전에 소개했던 사례들과는 다른 내용을 다루기 때문에, 이전 강의에서 다루었던 **주제**에 대해서만 간략하게 다룰 예정입니다.\n",
        "\n",
        "연합학습은 본질적으로 원시 클라이언트 데이터 대신 모델 업데이트만 서버로 전송하여 개인정보를 보호하는 형식을 갖추고 있습니다.\n",
        "\n",
        "그러나, 예를 들어서 훈련 데이터의 일부를 재구성하거나, 훈련된 모델에게만 주어진 특정 사용자의 구성원 자격을 추론할 수 있는 공격이 존재하는 것으로 밝혀졌습니다.\n",
        "\n",
        "이러한 보안 위험을 완화하기 위해 사용되는 일반적인 접근 방식은 차등 개인정보보호입니다.\n",
        "\n",
        "이번 튜토리얼에서는 차등 개인정보보호에 대한 개념을 소개하고 [Opacus library](https://opacus.ai/)를 사용하여 Flower 클라이언트에 추가하는 방법을 알려드리겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74kdmRhoOoa_"
      },
      "source": [
        "## 차등 개인정보보호가 무엇인가요?\n",
        "​\n",
        "차등 개인정보보호란 모델의 파라미터와 임의의 측면 정보가 주어진 사용자에 대한 공격자의 학습 정보 위험에 대해 수학적으로 추론할 수 있는 개인정보보호에 대한 정의입니다.\n",
        "\n",
        "차등 개인정보보호를 만족시키기 위한 훈련 알고리즘은 모델 훈련 과정 누락되거나 나타나는 단일 사용자가 모델에 너무 많은 영향을 미치지 않도록 해야 합니다.\n",
        "​\n",
        "형식적으로, ([Dwork et al. 2006](https://www.iacr.org/archive/eurocrypt2006/40040493/40040493.pdf))에 인접한 두 개의 데이터 세트 $d$ 와 $d'$가 주어지며,\n",
        "\n",
        "여기서 단일 사용자에 속하는 $d$에서 모든 샘플을 추가하거나 제거함으로써 $d$로부터 $d'$를 생성할 수 있는\n",
        "\n",
        "임의 훈련 알고리즘 $\\mathcal{A}: \\mathcal{D} \\rightarrow \\mathcal{M}$은 사용 가능한 훈련 데이터 세트를 사용 가능한 훈련 모델에 매핑하면$(\\varepsilon, \\delta)$,\n",
        "\n",
        "차등 개인정보보호의 경우, 임의의 하위 데이터 세트 $O \\in \\mathcal{M}$에 대해 다음 수식을 얻을 수 있습니다.\n",
        "\n",
        "$$Pr[\\mathcal{A}(d) \\in O] \\leq e^\\varepsilon Pr[\\mathcal{A}(d') \\in O] + \\delta.$$\n",
        "\n",
        "여기서 $\\varepsilon$는 소위 개인정보보호 비용으로 정의할 수 있습니다.\n",
        "\n",
        "낮을수록 개인정보보호 보장을 제공할 수 있습니다.\n",
        "\n",
        "또 다른 파라미터인 $\\delta$는 예기치 못한 상황이 발생하여 해당 보증을 이행할 수 없을 확률의 척도(낮을수록 좋음)으로 정의할 수 있습니다.\n",
        "\n",
        "딥러닝의 경우 $(\\varepsilon, \\delta)$ 차등 개인정보보호를 달성하기 위해 현재 가장 일반적으로 사용되는 알고리즘은\n",
        "\n",
        "DP-SGD ([Abadi et al. 2016](https://arxiv.org/pdf/1607.00133.pdf))로, FedAvg ([McMahan et al. 2017](https://arxiv.org/pdf/1710.06963v1.pdf))와 같은 연합학습 전략의 일부로 쉽게 사용할 수 있습니다.\n",
        "​\n",
        "\n",
        "DP-SGD에는 다음과 같이 사용자 개인정보보호를 향상시키는 데 사용되는 두 가지 주요 단계가 있습니다:​\n",
        "​\n",
        "- **Gradient norm clipping**은 전체 평균에 대한 단일 클라이언트의 영향을 보장하는 것으로, 모델과 데이터에 의해 결정되는 많은 요인 $L$의 최대 기울기 정규화로 제한합니다.\n",
        "\n",
        "- **Gaussian noising**은 모델의 파라미터에 $\\mathcal{N}(0, L^2\\sigma^2)$를 추가하여 필요한 무작위성 요소를 삽입합니다(여기서 $\\sigma$는 노이즈와 관련됨).\n",
        "​\n",
        "\n",
        "또한, 이 알고리즘은 임의로 균일하게 선택되는 클라이언트와 데이터 샘플에 의존하는 특성이 존재합니다.\n",
        "\n",
        "특히 연합학습의 경우, 서버로 모델 파라미터를 전송하기 전에 모델 업데이트에 클라이언트 단에서 노이즈를 추가하거나 서버 단에서 노이즈를 추가할 수 있습니다.\n",
        "\n",
        "중앙집중형 방식은 전체적으로 노이즈가 적기 때문에 보다 정확한 모델을 유도하지만, 신뢰할 수 있는 서버가 존재한다는 가정 하에 이루어집니다.\n",
        "\n",
        "다음 예시는 클라이언트 단에서 차등 개인정보보호에 초점을 맞춥니다.\n",
        "\n",
        "차등 개인정보보호 메커니즘을 제공하는 라이브러리의 또다른 중요한 부분은 개인정보보호 비용을 추적할 수 있는 방법을 제공하는 것입니다.\n",
        "\n",
        "이는 동일한 데이터 세트에 대한 메커니즘의 여러 응용 프로그램이 제공 가능한 개인정보보호 보장을 변경하기 때문입니다.\n",
        "\n",
        "데이터에 대한 액세스는 잠재적으로 클라이언트에 대한 더 많은 정보를 노출시키는 것을 의미합니다.\n",
        "\n",
        "$\\varepsilon$ 값과 경계를 계산하는 것은 수학적으로 매우 복잡하지만,\n",
        "\n",
        "일반적으로 클라이언트에서 수행되는 모든 훈련 단계에 따라 점진적으로 $\\varepsilon$가 증가하는 결과를 초래하는 반면,\n",
        "\n",
        "연합학습 설정을 통한다면 모든 클라이언트 중에서 최대 $\\varepsilon$가 전쳬 비용을 결정하는 병렬 구성의 형태를 띄게 될 것입니다.\u001c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWuQxW0vOobA"
      },
      "source": [
        "## Opacus 설치\n",
        "\n",
        "Opacus를 포함한 필요한 종속성을 설치하고 불러오는 것으로 시작해볼까요?\n",
        "\n",
        "현재 Opacus의 출시 버전은 최신 버전의 PyTorch와 호환되지 않습니다.\n",
        "\n",
        "따라서, 이전 버전의 `torch` 와 `torchvision`을 설치하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U4vQwxtOobB"
      },
      "source": [
        "!pip install torchcsprng==0.1.3+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install matplotlib opacus==0.14.0 torch==1.7.0 torchvision==0.8.0 git+https://github.com/adap/flower.git@release/0.17#egg=flwr[\"simulation\"]\n",
        "\n",
        "from collections import OrderedDict\n",
        "from typing import List\n",
        "\n",
        "import flwr as fl\n",
        "import numpy as np\n",
        "import opacus                                           # <-- NEW\n",
        "from opacus import PrivacyEngine                        # <-- NEW\n",
        "from opacus.dp_model_inspector import DPModelInspector  # <-- NEW\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)\n",
        "print(opacus.__version__)\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "DEVICE = \"cpu\"  # Enable this line to force execution on CPU\n",
        "print(f\"Training on {DEVICE}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qEtQDvZOobC"
      },
      "source": [
        "## 작업 정의\n",
        "\n",
        "이전과 마찬가지로 기본적인 작업을 정의하는 것으로 시작하겠습니다.\n",
        "\n",
        "여기에는 데이터 불러오기 및 모델 구성과 `get_parameters`/`set_parameters`가 포함됩니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-ExlfXQOobD"
      },
      "source": [
        "NUM_CLIENTS = 10\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def load_datasets():\n",
        "    # Download and transform CIFAR-10 (train and test)\n",
        "    transform = transforms.Compose(\n",
        "      [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "    )\n",
        "    trainset = CIFAR10(\"./dataset\", train=True, download=True, transform=transform)\n",
        "    testset = CIFAR10(\"./dataset\", train=False, download=True, transform=transform)\n",
        "\n",
        "    # Split training set into 10 partitions to simulate the individual dataset\n",
        "    partition_size = len(trainset) // NUM_CLIENTS\n",
        "    lengths = [partition_size] * NUM_CLIENTS\n",
        "    datasets = random_split(trainset, lengths, torch.Generator().manual_seed(42))\n",
        "\n",
        "    # Split each partition into train/val and create DataLoader\n",
        "    trainloaders = []\n",
        "    valloaders = []\n",
        "    for ds in datasets:\n",
        "        len_val = len(ds) // 10  # 10 % validation set\n",
        "        len_train = len(ds) - len_val\n",
        "        lengths = [len_train, len_val]\n",
        "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(42))\n",
        "        trainloaders.append(DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True))\n",
        "        valloaders.append(DataLoader(ds_val, batch_size=BATCH_SIZE))\n",
        "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
        "    return trainloaders, valloaders, testloader\n",
        "\n",
        "trainloaders, valloaders, testloader = load_datasets()\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def get_parameters(net) -> List[np.ndarray]:\n",
        "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
        "\n",
        "def set_parameters(net, parameters: List[np.ndarray]):\n",
        "    params_dict = zip(net.state_dict().keys(), parameters)\n",
        "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "    net.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "\n",
        "print(len(trainloaders[0]))\n",
        "print(len(valloaders[0]))\n",
        "print(len(trainloaders[0].dataset))\n",
        "print(len(valloaders[0].dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNRj4TtOOobD"
      },
      "source": [
        "## Opacus 예제\n",
        "​\n",
        "차등 개인정보보호를 보장하는 기계학습을 구현할 수 있는 여러 라이브러리가 있습니다.\n",
        "\n",
        "이러한 것들은 일반적으로 클리핑 단계가 일반적으로 각 훈련 단계와 함께 수행되므로,\n",
        "\n",
        "최적화를 위한 라이브러리별 래퍼를 가져야 하기 때문에 모델을 훈련하는 데 사용하는 라이브러리에 따라 달라지게 됩니다.\n",
        "\n",
        "이번 예제는 PyTorch에 최적화된 Opacus를 사용하지만 [Tensorflow Privacy를 사용하여 DP-SGD를 구현하는 Flower 클라이언트](https://github.com/adap/flower/tree/main/examples/dp-sgd-mnist)도 있습니다.\n",
        "​\n",
        "\n",
        "또한 이번 예제는 [PyTorch Quickstart](https://flower.dev/docs/quickstart_pytorch.html)에 기반을 두고 있습니다(여러분들이 PyTorch에 익숙하고 코드도 다루실줄 안다고 믿고 있습니다). \n",
        "​\n",
        "첫 번째 단계는 네트워크가 현재 모든 계층을 지원하지 않기 때문에 네트워크가 Opacus와 호환되는지 확인하는 것입니다(자세한 내용은 [문서](https://github.com/pytorch/opacus/blob/master/opacus/README.md)를 확인해보시길 바랍니다).\n",
        "\n",
        "이를 위해 모델을 인스턴스화할 때 `DPModelInspector`를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDsJ9yr2OobE"
      },
      "source": [
        "def validate_model():\n",
        "  model = Net()\n",
        "  inspector = DPModelInspector()\n",
        "  print(inspector.validate(model))\n",
        "\n",
        "\n",
        "validate_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BYE4IK7OobG"
      },
      "source": [
        "설정에서 가장 까다로운 부분은 개인정보보호 엔진에 대한 올바른 개인정보보호 파라미터를 찾는 것입니다.\n",
        "\n",
        "- **Target delta $\\delta$**: 훈련 데이터 세트의 크기의 역으로 설정해야 합니다. 예를 들어, 데이터 세트에 CIFAR10과 같은 50,000개의 훈련 데이터가 있는 경우, 이를 설정하기 좋은 수치는 $10^{-5}$ 입니다.\n",
        "- **Noise multiplier $\\sigma$**: 각 단계에서 추가된 노이즈의 양을 측정하면, $\\varepsilon$의 값이 작을수록 커집니다.\n",
        "- **Target epsilon $\\varepsilon$**: \n",
        "고정된 **Noise multiplier**의 대안으로, 목표 $\\varepsilon$가 주어지면 엔진에 의해 계산될 수 있습니다. 그러나 이 변수는 전역 훈련과 지역 훈련 라운드를 모두 고려해야 하기 때문에 연합학습 환경에서 알아내기 어려운 훈련 단계를 제공하도록 요구합니다.\n",
        "- **Maximum gradient norm $L$**: 모델 설계, 클라이언트에 대한 훈련 데이터 양 및 학습 속도와 같은 요인에 크게 의존하는 이 파라미터의 경우, 너무 낮게 설정하면 높은 바이어스를 초래할 수 있고, 너무 높게 설정하면 모델 유틸리티가 손상될 수 있기 때문에 그리드 검색을 하는 것이 유용할 수 있습니다([Andrew et al. 2021](https://arxiv.org/pdf/1905.03871.pdf)).\n",
        "\n",
        "샘플 비율 대신 `sample_rate = batch_size / sample_size`이므로, `batch_size`(한 단계에서 취한 훈련 샘플 수)와 `sample_size`(한 클라이언트의 데이터 세트 전체 크기)를 모두 제공할 수도 있습니다.\n",
        "\n",
        "이 외에도 고급 파라미터를 지정할 수 있으며, [문서](https://opacus.ai/api/privacy_engine.html)와 [튜토리얼](https://opacus.ai/tutorials/)에서 더욱 자세한 정보를 찾을 수 있습니다.\n",
        "\n",
        "이번 예제는 다음과 같은 파라미터를 사용하지만 최적화된 과정은 아닙니다(utility-privacy trade-off 측면에서 선호도에 따라 상이함)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWqvWM_bOobG"
      },
      "source": [
        "PARAMS = {\n",
        "    'batch_size': 32,\n",
        "    'train_split': 0.7,\n",
        "    'local_epochs': 1\n",
        "}\n",
        "PRIVACY_PARAMS = {\n",
        "    'target_delta': 1e-5,\n",
        "    'noise_multiplier': 0.4,\n",
        "    'max_grad_norm': 1.2\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEVxJ7SNOobG"
      },
      "source": [
        "마지막 단계는 각 훈련에서 옵티마이저에 개인정보보호 엔진을 부착하고\n",
        "\n",
        "선택적으로 지출된 현재 개인정보보호 비용을 받아 반환하는 것입니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ba4IbqeOobH"
      },
      "source": [
        "def train(net, trainloader, privacy_engine, epochs):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    # Attach privacy engine to optimizer\n",
        "    privacy_engine.attach(optimizer)\n",
        "    for _ in range(epochs):\n",
        "        for images, labels in trainloader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(net(images), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    # Get privacy budget\n",
        "    epsilon, _ = optimizer.privacy_engine.get_privacy_spent(PRIVACY_PARAMS['target_delta'])\n",
        "    return epsilon\n",
        "\n",
        "\n",
        "# Same as before\n",
        "def test(net, testloader):\n",
        "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    correct, total, loss = 0, 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    loss /= len(testloader.dataset)\n",
        "    accuracy = correct / total\n",
        "    return loss, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBFybDO-OobE"
      },
      "source": [
        "다음은 클라이언트의 `PrivacyEngine` 사례입니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L94CgwsOobF"
      },
      "source": [
        "PE = {}\n",
        "\n",
        "def get_privacy_engine(cid, model, sample_rate):\n",
        "    if cid not in PE.keys():\n",
        "        PE[cid] = PrivacyEngine(\n",
        "            model,\n",
        "            sample_rate = sample_rate,\n",
        "            target_delta = PRIVACY_PARAMS['target_delta'],\n",
        "            max_grad_norm = PRIVACY_PARAMS['max_grad_norm'],\n",
        "            noise_multiplier = PRIVACY_PARAMS['noise_multiplier']\n",
        "        )\n",
        "    return PE[cid]  # Use the previously created PrivacyEngine"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdpa-TxgQ45m"
      },
      "source": [
        "개인정보추적이 정확하지 않기 때문에 모델당 각 훈련 내내 유지되는 하나의 엔진을 보유하는 것이 중요합니다.\n",
        "\n",
        "자원적으로 효울적인 단일 기기 시뮬레이션을 위해 Flower 가상 클라이언트 엔진을 사용하면서도 약간의 트릭을 써야합니다.\n",
        "\n",
        "각 클라이언트에 대한 개인정보보호 엔진 초기화를 지연하고 딕셔너리에 참조를 유지해야 합니다.\n",
        "\n",
        "이를 통해 가상 클라이언트 엔진을 사용 후에 `FlowerClient` 인스턴스를 삭제할 수 있지만,\n",
        "\n",
        "여전히 각 클라이언트에 대해 매번 동일한 `PrivacyEngine` 인스턴스를 재사용하게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yjAh9GpOobH"
      },
      "source": [
        "## FlowerClient 구현\n",
        "\n",
        "그런 다음, 클라이언트의 훈련 함수에서 개인정보보호 비용을 사용자 지정 메트릭으로 반환할 수도 있습니다\n",
        "\n",
        "([연합학습 집계 전략 오버라이딩](https://flower.dev/docs/saving-progress.html)에 의해 더 다양하게 사용할 수 있습니다)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F47ILSzNOobH"
      },
      "source": [
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, net, trainloader, valloader, privacy_engine) -> None:\n",
        "        super().__init__()\n",
        "        self.net = net\n",
        "        self.trainloader = trainloader\n",
        "        self.valloader = valloader\n",
        "        self.privacy_engine = privacy_engine\n",
        "\n",
        "    def get_parameters(self):\n",
        "        return get_parameters(self.net)\n",
        "    \n",
        "    def fit(self, parameters, config):\n",
        "        set_parameters(self.net, parameters)\n",
        "        epsilon = train(self.net, self.trainloader, self.privacy_engine, PARAMS['local_epochs'])\n",
        "        print(f\"epsilon = {epsilon:.2f}\")\n",
        "        return get_parameters(self.net), len(self.trainloader), {\"epsilon\":epsilon}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        set_parameters(self.net, parameters)\n",
        "        loss, accuracy = test(self.net, self.valloader)\n",
        "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqdlVQ2_OobI"
      },
      "source": [
        "## 학습 시작\n",
        "\n",
        "이젠 전에 하셨던 것처럼 `FlowerClient`를 사용할 수 있습니다!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hdHdEAROobI"
      },
      "source": [
        "def client_fn(cid) -> FlowerClient:\n",
        "    \"\"\"Create a Flower client representing a single organization.\"\"\"\n",
        "\n",
        "    # Load model\n",
        "    net = Net().to(DEVICE)\n",
        "\n",
        "    # Load data (CIFAR-10)\n",
        "    trainloader = trainloaders[int(cid)]\n",
        "    valloader = valloaders[int(cid)]\n",
        "\n",
        "    # PrivacyEngine\n",
        "    sample_rate = BATCH_SIZE / len(trainloader.dataset) \n",
        "    pe = get_privacy_engine(cid, net, sample_rate)\n",
        "\n",
        "    # Create a  single Flower client representing a single organization\n",
        "    return FlowerClient(net, trainloader, valloader, pe)\n",
        "\n",
        "\n",
        "# Create FedAvg strategy\n",
        "strategy = fl.server.strategy.FedAvg(\n",
        "        fraction_fit=1.0,  # Sample 100% of available clients for training\n",
        "        fraction_eval=1.0,  # Sample 100% of available clients for evaluation\n",
        "        min_fit_clients=10,  # Never sample less than 10 clients for training\n",
        "        min_eval_clients=10,  # Never sample less than 10 clients for evaluation\n",
        "        min_available_clients=10,  # Wait until all 10 clients are available\n",
        ")\n",
        "\n",
        "# Start simulation\n",
        "fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    num_rounds=5,\n",
        "    strategy=strategy,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuniVKKGOobI"
      },
      "source": [
        "## 한계점 및 개선점\n",
        "​\n",
        "이와 같은 기본 설정을 구현하는 것은 비교적 간단하지만,\n",
        "\n",
        "실제로 연합학습 환경에서 차등 개인정보보호 모델을 배치하는 것에 관해서는 많은 고려사항과 질문들이 있습니다.\n",
        "\n",
        "위의 사항들은 궁극적으로 유용성과 개인정보보호 사이의 절충안을 고려하는 것으로 결론지을 수 있습니다.\n",
        "\n",
        "한편, 차등 개인정보보호 모델은 수렴하는데 더욱 오랜 시간이 소요되며, 더 많은 계산이 필요하고([McMahan et al. 2017](https://arxiv.org/pdf/1710.06963v1.pdf)), 더 나쁜 모델을 초래할 수 있습니다([Bagdasaryan et al. 2019](https://proceedings.neurips.cc/paper/2019/file/fc0de4e0396fff257ea362983c2dda5a-Paper.pdf)).\n",
        "\n",
        "반면에 상대적으로 큰 개인정보보호 비용은 사용자 개인정보보호에 유리합니다([Thakkar et al. 2020](https://arxiv.org/pdf/2006.07490.pdf)).\n",
        "\n",
        "따라서 필요한 유틸리티 및 개인정보보호에 대한 개인정보보호 파라미터 및 가정을 고려하는 것이 중요합니다.\n",
        "\n",
        "Flower에서 다음 단계는 기계 학습 라이브러리와 독립되도록 차등 개인정보보호를 적용한 방법으로 모델을 훈련시키는 방법을 찾는 것이지만,\n",
        "\n",
        "​이 예시에서도 연합학습에서 개인정보보호를 실험하는 유용한 첫 번째 단계가 되기를 바랍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5KLdJVvaDwD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
